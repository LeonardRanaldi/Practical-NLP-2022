{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizzazione & WordNet (NLTK)\n",
        "\n",
        "NLTK (libreria Python completa per l'NLP e l'analisi del testo).\n",
        "\n",
        "\n",
        "Tokenizzazione: metodo per suddividere un testo in più parti, come frasi e parole, ed è un primo passo essenziale per i progetti di NLP.\n",
        "\n",
        "Consiste nel suddividere una stringa in un elenco di pezzi o token. \n",
        "\n",
        "Un token è un pezzo di un insieme, quindi una parola è un token in una frase e una frase è un token in un paragrafo. \n",
        "\n",
        "\n",
        "**WordNet** è un dizionario progettato per l'accesso programmatico da parte dei sistemi di elaborazione del linguaggio naturale. \n",
        "\n",
        "Ha molti casi d'uso diversi, tra cui: \n",
        " - Trovare sinonimi e contrari di una parola;\n",
        " - Esplorare le relazioni e le somiglianze tra le parole;\n",
        " - Cercare la definizione di una parola;\n",
        " - Disambiguazione del senso della parola per parole che hanno usi e definizioni multiple;\n"
      ],
      "metadata": {
        "id": "skUc5RfShqYH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK include un lettore di corpus **WordNet**, che verrà usato per accedere ed esplorare WordNet. \n",
        "\n",
        "*Corpus:* insieme di testi, e i lettori di corpus sono progettati per rendere l'accesso a un corpus molto più semplice rispetto all'accesso diretto alle fle."
      ],
      "metadata": {
        "id": "pI9afoIRqXuH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NLTK (tokenization) "
      ],
      "metadata": {
        "id": "Nz2IeO-L4uhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWI_EE6EqqiR",
        "outputId": "029f2e68-c642-43c1-ebaa-c200131a7fa8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can start by creating a paragraph of text:\n",
        "para = \"Good afternoon, everyone. I am the NLP professor today! This lecture is interesting?\"\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "sent_tokenize(para)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTfUyzvmqWo0",
        "outputId": "bbf9da42-6832-41b0-f55d-b09d42f92783"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Good afternoon, everyone.',\n",
              " 'I am the NLP professor today!',\n",
              " 'This lecture is interesting?']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La funzione **sent_tokenize** utilizza un'istanza di **PunktSentenceTokenizer** dal modulo *nltk.tokenize.punkt*.\n",
        "\n",
        "Questa istanza è già stata addestrata e funziona bene per molte lingue europee, quindi sa quindi quali sono i caratteri di punteggiatura che segnano la fine di una frase e l'inizio di una nuova frase.\n",
        "\n",
        "**Tokenizzazione di frasi in altre lingue**\n",
        "Se si desidera tokenizzare frasi in lingue diverse dall'inglese, si può caricare uno degli altri pickle in tokenizers/punkt/PY3 e usarlo proprio come il tokenizzatore di frasi in inglese. "
      ],
      "metadata": {
        "id": "LRqCR64urFXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ita_tokenizer = nltk.data.load('tokenizers/punkt/italian.pickle')\n",
        "ita_tokenizer.tokenize('Che giornataccia. Ho molto sonno!')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8LJcrPTqoK8",
        "outputId": "8490ab48-e213-4ad0-c8cd-a0d4e7d78173"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Che giornataccia.', 'Ho molto sonno!']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizzazione delle frasi in parole**\n",
        "Si può fare un ulteriore divisione, ad esempio quella di una frase in singole parole. "
      ],
      "metadata": {
        "id": "auMozv2ssEIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize('Good afternoon,, everyone.!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLMtgnxkrjtu",
        "outputId": "29f4d100-8c9f-434b-b14f-5ea1d7a3a4a7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Good', 'afternoon', ',', ',', 'everyone', '.', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La funzione **word_tokenize()** è una funzione wrapper che chiama **tokenize()** su un'istanza della classe *TreebankWordTokenizer*. \n",
        "\n",
        "Funziona separando le parole con spazi e punteggiatura. Come si può vedere, non scarta la punteggiatura, lasciando all'utente la possibilità di decidere cosa farne. "
      ],
      "metadata": {
        "id": "Q8uogxxMshno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokenizer.tokenize('Good afternoon,, everyone.!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdslwiYZsXwo",
        "outputId": "3ff0a41e-e157-441f-ffd7-da3ea1c2e2a5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Good', 'afternoon', ',', ',', 'everyone.', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un altro tokenizzatore di parole alternativo è **WordPunctTokenizer**.\n",
        "\n",
        "Divide tutta la punteggiatura in token separati:"
      ],
      "metadata": {
        "id": "gnoOr-bqs4xN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "tokenizer = WordPunctTokenizer()\n",
        "tokenizer.tokenize('Good afternoon,, everyone.!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKRN5nJMs1VZ",
        "outputId": "b00fd8a6-ebe6-43a4-8bc4-fb0e1e149b8e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Good', 'afternoon', ',,', 'everyone', '.!']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizzare le frasi usando le espressioni regolari**\n",
        "Le espressioni regolari possono essere utilizzate se si vuole avere un controllo completo su come tokenizzare il testo. Poiché le espressioni regolari possono diventare complicate molto rapidamente, ne raccomando l'uso solo se i tokenizzatori di parole trattati nella ricetta precedente non sono accettabili.\n"
      ],
      "metadata": {
        "id": "KjqUk2S5tL7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
        "tokenizer.tokenize('Good afternoon,, everyone.!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buBZmImus5Ou",
        "outputId": "edb690a6-b13b-4b63-bacf-d0ef2909a4c5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Good', 'afternoon', 'everyone']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize(\"Today isn't good day!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5TKry_ftuKX",
        "outputId": "923f5e0b-e6c3-4199-bc3c-15edb4539a43"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Today', \"isn't\", 'good', 'day']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
        "tokenizer.tokenize(\"Today isn't good day!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQudJlObtaSp",
        "outputId": "ceeaa636-2873-406b-a7c9-fafdeb439c64"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Today', \"isn't\", 'good', 'day']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Filtrare le stopword in una frase tokenizzata**\n",
        "Le **stopword** sono parole comuni che generalmente non contribuiscono al significato di una frase, almeno ai fini del reperimento delle informazioni e dell'NLP. \n",
        "Infatti la maggior parte dei motori di ricerca elimina le stopword dalle query di ricerca e dai documenti per risparmiare spazio nel loro indice.\n",
        "\n",
        "\n",
        "In NLTK viene fornito con un corpus di stopwords che contiene elenchi di parole per molte lingue. "
      ],
      "metadata": {
        "id": "rO90SP8lt4EG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "english_stops = set(stopwords.words('english'))\n",
        "words = ['Good', 'afternoon', 'for', 'everyone']\n",
        "[word for word in words if word not in english_stops]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aupya3i_tr6I",
        "outputId": "b907144d-a302-4401-daf5-950b4ce718e3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Good', 'afternoon', 'everyone']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "english_stops = set(stopwords.words('italian'))\n",
        "words = ['Che', 'giornata', 'piovosa', 'è', 'rilassante']\n",
        "[word for word in words if word not in english_stops]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoXfPoduusqw",
        "outputId": "65e9a29d-8884-4bb4-cd14-20a9e67d7a1c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Che', 'giornata', 'piovosa', 'rilassante']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il corpus **stopwords** è un'istanza di *nltk.corpus.reader.WordListCorpusReader*. \n",
        "\n",
        "Il metodo *words()* può accettare un singolo argomento per l'ID del fle, che in questo caso è 'italian', riferendosi a un fle contenente un elenco di stopwords italiane."
      ],
      "metadata": {
        "id": "_gyCtDTQvDFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#WordNet "
      ],
      "metadata": {
        "id": "BW-Uxl0M4nXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WordNet** https://wordnet.princeton.edu/\n",
        "\n",
        "è un database lessicale per la lingua inglese. \n",
        "In altre parole, è un dizionario progettato specificamente per l'NLP. \n",
        "\n",
        "NLTK è dotato di una semplice interfaccia per cercare le parole in WordNet. \n",
        "\n",
        "Si ottiene un elenco di istanze di *Synset*, che sono raggruppamenti di parole sinonime che esprimono lo stesso concetto. Molte parole hanno un solo *Synset*, ma alcune ne hanno diversi. \n",
        "\n",
        "*NB* Per ora esploreremo un singolo *Synset*\n"
      ],
      "metadata": {
        "id": "zcA09jjqvZjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "syn = wordnet.synsets('cocaine')[0]\n",
        "syn.name()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "tNLBl4bFulUz",
        "outputId": "30c06d64-107d-4d05-f132-f2de3d11a50d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cocaine.n.01'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "syn.definition()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iwlkps9mxiL_",
        "outputId": "b5d92875-1b49-4c64-ad63-2b693b294809"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a narcotic (alkaloid) extracted from coca leaves; used as a surface anesthetic or taken for pleasure; can become powerfully addictive'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "È possibile cercare qualsiasi parola in WordNet utilizzando **wordnet.synset (word)** per ottenere un elenco di sinonimi. \n",
        "\n",
        "L'elenco può essere vuoto se la parola non viene trovata. \n",
        "\n",
        "L'elenco può anche avere molti elementi, poiché alcune parole possono avere molti significati possibilivquindi molti insiemi."
      ],
      "metadata": {
        "id": "W1yJsRl7UWMz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**lemmi e sinonimi**\n",
        "\n",
        "Possiamo anche cercare i lemmi per trovare i sinonimi di una parola. \n",
        "\n",
        "(*lemma:* forma canonica o morfologica di una parola)."
      ],
      "metadata": {
        "id": "uk49k1Dxdwo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "syn = wordnet.synsets('cookbook')[0]\n",
        "lemmas = syn.lemmas()\n",
        "len(lemmas)"
      ],
      "metadata": {
        "id": "l9wTEJyjxpsX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47d4a57a-8bf8-49b1-ccd5-eef08c1d27d6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmas[0].name()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HVgJP9aTUYF3",
        "outputId": "e1ce65ce-d27b-4723-f77f-dcb66a4be470"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cookbook'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmas[1].name()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7CrHxbRIUgdX",
        "outputId": "6e2949eb-3da7-4a57-d5f9-cc98a808b5c6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cookery_book'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmas[0].synset() == lemmas[1].synset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_Hi_KmEUi09",
        "outputId": "e9d11704-0abc-46de-f52f-d7bec7f3dbae"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Come si può vedere, cookery_book e cookbook sono due lemmi distinti nello stesso Synset.\n",
        "\n",
        "Infatti, un lemma può appartenere a un solo Synset.\n",
        "\n",
        "**In questo modo, un Synset rappresenta un gruppo di lemmi che hanno tutti lo stesso significato, mentre un lemma rappresenta una forma di parola distinta.**\n",
        "\n",
        "\n",
        "Poiché tutti i lemmi di un Synset hanno lo stesso significato, possono essere trattati come sinonimi. \n",
        "\n",
        "Quindi, se si vogliono ottenere tutti i sinonimi di un Synset, si può procedere come segue:"
      ],
      "metadata": {
        "id": "VbKvMhnJUn3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[lemma.name() for lemma in syn.lemmas()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "754FhWEGUlXH",
        "outputId": "cbdb1276-fd81-4fdb-fe64-17c7b0c54f5f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cookbook', 'cookery_book']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "GI0gHRW0Upmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "synonyms = []\n",
        "for syn in wordnet.synsets('book'):\n",
        "    for lemma in syn.lemmas():\n",
        "        synonyms.append(lemma.name())\n",
        "len(synonyms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WpGfC-nUocG",
        "outputId": "570eaddd-bcf8-494c-c8ec-c700b550b2db"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Come si può vedere, sembrano esserci 38 possibili sinonimi per la parola **\"book\"**. \n",
        "\n",
        "In realtà, alcuni sinonimi sono forme verbali e molti sinonimi sono solo usi diversi di \"libro\". Se invece prendiamo l'insieme dei sinonimi, le parole uniche sono meno numerose, come mostra il codice seguente:"
      ],
      "metadata": {
        "id": "i4QV-SojUwx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(synonyms))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eR3MMFpUvsu",
        "outputId": "a9883b71-cba2-402b-edf6-40520d4f0718"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alcuni lemmi hanno anche degli **antonimi**. \n",
        "\n",
        "La parola **\"good\"**, per esempio, ha 27 sintagmi, cinque dei quali hanno lemmi con antonimi, come mostrato nel codice seguente:"
      ],
      "metadata": {
        "id": "PE-LTSrpU0cr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gn2 = wordnet.synset('good.n.02')\n",
        "gn2.definition()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qFdhyeH4Uzab",
        "outputId": "9b19e80e-64c5-4982-e7e7-055560616a9a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'moral excellence or admirableness'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evil = gn2.lemmas()[0].antonyms()[0]\n",
        "evil.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHt0juPLU2kY",
        "outputId": "c304fd77-e5b5-4d7c-9ec2-c211b1561281"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Lemma.name of Lemma('evil.n.03.evil')>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evil.synset().definition()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YLs0Evi0U6OI",
        "outputId": "c950f2a2-fb03-4f6d-c102-13555f7188d5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the quality of being morally wrong in principle or practice'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ga1 = wordnet.synset('good.a.01')\n",
        "ga1.definition()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "N3tWatqyU9s9",
        "outputId": "cb13ccc0-39d9-4585-af31-f4fd26e459e2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'having desirable or positive qualities especially those suitable for a thing specified'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bad = ga1.lemmas()[0].antonyms()[0]\n",
        "bad.name()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "S3NkesV1VAf3",
        "outputId": "06ac76ca-a298-4a38-f399-19b99ace12e0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bad'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bad.synset().definition()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pClP7PduVDP0",
        "outputId": "9ec8178d-c479-483a-fa49-7070d0df5130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'having undesirable or negative qualities'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il metodo *antonyms()* restituisce un elenco di lemmi. \n",
        "\n",
        "Nel primo caso, come si può vedere nel codice precedente, il secondo sintagma di **\"good\"** come sostantivo è definito come eccellenza morale e il suo primo antonimo è evil, definito come moralmente sbagliato. \n",
        "\n",
        "Nel secondo caso, quando good è usato come aggettivo per descrivere qualità positive, il primo antonimo è bad, che descrive qualità negative."
      ],
      "metadata": {
        "id": "XsE2SV0_VHYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Somiglianza tra i sinonimi di WordNet**\n",
        "\n",
        "I sinonimi sono organizzati in un albero di **hypernym**. \n",
        "\n",
        "Questo albero può essere utilizzato per ragionare sulla somiglianza tra i sinonimi che contiene. \n",
        "\n",
        "*Più i due sinonimi sono vicini nell'albero, più sono simili.*\n",
        "\n",
        "\n",
        "Se si esaminassero tutti gli iponimi di reference_book (che è l'iperonimo di cookbook), si vedrebbe che uno di essi è instruction_book.....\n",
        "\n",
        "\n",
        "Questo sembra intuitivamente molto simile a un libro di cucina, quindi vediamo cosa dice la similarità di WordNet al riguardo con l'aiuto del seguente codice:"
      ],
      "metadata": {
        "id": "8hNHTnrd2ffc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "cb = wordnet.synset('cookbook.n.01')\n",
        "ib = wordnet.synset('instruction_book.n.01')\n",
        "cb.wup_similarity(ib)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WpUNEbjVGs-",
        "outputId": "429df013-2fd9-4e55-9c13-0c0409b8acff"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9166666666666666"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*wup_similarity()* proviene da Wu-Palmer Similarity, che è un metodo di punteggio basato sulla somiglianza dei sensi delle parole e sulla posizione dei sinonimi nell'albero degli ipernimi **hypernym tree**. \n",
        "\n",
        "\n",
        "Una delle metriche principali utilizzate per calcolare la somiglianza è la shortest path tra i due sinonimi e il loro iperonimo comune:"
      ],
      "metadata": {
        "id": "sCvv2m48VKQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ref = cb.hypernyms()[0]\n",
        "cb.shortest_path_distance(ref)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEsnbWDeVJb-",
        "outputId": "5e0e9aac-ffcb-499b-93d5-2f331d285219"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quindi **cookbook** e **instruction_book** devono essere molto simili, perché sono a un passo dallo stesso iperonimo  e, quindi, a due passi l'uno dall'altro."
      ],
      "metadata": {
        "id": "_ulFZ4hR4GF1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pa2H6B3XVMRc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}